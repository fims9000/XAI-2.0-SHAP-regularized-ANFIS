"""
SHAP-—Ä–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä ANFIS
"""
import time
import torch
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import (accuracy_score, roc_auc_score, precision_score, recall_score, f1_score,
                           mean_squared_error, mean_absolute_error, r2_score)

class ShapAwareANFISTrainer:
    """–¢—Ä–µ–Ω–µ—Ä ANFIS —Å SHAP-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π"""

    def __init__(self, model,config, gamma=0.5, verbose=True):
        self.model = model.network
        self.gamma = gamma
        self.verbose = verbose
        self.task_type = config['dataset']['task_type']
        self.training_time = 0

    def fit(self, X_train, y_train, epochs=25, batch_size=32, lr=0.005):
        """–û–±—É—á–µ–Ω–∏–µ —Å SHAP-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π"""
        start_time = time.time()

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_tensor = torch.tensor(X_train, dtype=torch.float32)
        y_tensor = torch.tensor(y_train, dtype=torch.float32)
        training_dataset = TensorDataset(X_tensor, y_tensor)
        data_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)

        # –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è SHAP
        baseline_values = np.mean(X_train, axis=0)

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)

        # –í—ã–±–æ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
        if self.task_type == 'regression':
            loss_function = torch.nn.MSELoss()
        else:
            loss_function = torch.nn.BCELoss()

        # –ò—Å—Ç–æ—Ä–∏—è –ø–æ—Ç–µ—Ä—å
        history = {
            'total_loss': [],
            'main_loss': [],
            'shap_loss': []
        }

        if self.verbose:
            task_name = "—Ä–µ–≥—Ä–µ—Å—Å–∏–∏" if self.task_type == 'regression' else "–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"
            print(f"üü† –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ ANFIS —Å SHAP-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π ({task_name})...")

        for epoch in range(epochs):
            epoch_losses = {'total': [], 'main': [], 'shap': []}

            for batch_X, batch_y in data_loader:
                optimizer.zero_grad()

                # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
                self.model.train()
                predictions = self.model(batch_X).squeeze()
                main_loss = loss_function(predictions, batch_y)

                # SHAP —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
                shap_importance = self._calculate_shap_approximation(batch_X, baseline_values)
                shap_normalized = shap_importance / (np.sum(shap_importance) + 1e-8)
                target_uniform = np.ones_like(shap_normalized) / len(shap_normalized)
                shap_regularization_loss = np.mean((shap_normalized - target_uniform) ** 2)

                # –û–±—â–∞—è –ø–æ—Ç–µ—Ä—è
                total_loss = main_loss + self.gamma * shap_regularization_loss

                # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                total_loss.backward()
                optimizer.step()

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
                epoch_losses['total'].append(total_loss.item())
                epoch_losses['main'].append(main_loss.item())
                epoch_losses['shap'].append(shap_regularization_loss)

            # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å –ø–æ —ç–ø–æ—Ö–µ
            for loss_type in history:
                loss_key = loss_type.split('_')[0]
                history[loss_type].append(np.mean(epoch_losses[loss_key]))

            # –ü—Ä–æ–≥—Ä–µ—Å—Å
            if self.verbose and (epoch + 1) % 5 == 0:
                print(f"   –≠–ø–æ—Ö–∞ {epoch + 1}/{epochs}: "
                      f"Total: {history['total_loss'][-1]:.4f}, "
                      f"Main: {history['main_loss'][-1]:.4f}, "
                      f"SHAP: {history['shap_loss'][-1]:.4f}")

        self.training_time = time.time() - start_time

        if self.verbose:
            print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {self.training_time:.2f} —Å–µ–∫")

        return history

    def predict(self, X_test):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.tensor(X_test, dtype=torch.float32)
            predictions = self.model(X_tensor).squeeze().cpu().numpy()
            return predictions

    def get_global_shap_importance(self, X_sample):
        """–ì–ª–æ–±–∞–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        baseline_values = np.mean(X_sample, axis=0)
        return self._calculate_shap_approximation(X_sample, baseline_values)

    def _calculate_shap_approximation(self, X_batch, baseline):
        """–ü—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã–µ SHAP –∑–Ω–∞—á–µ–Ω–∏—è"""
        self.model.eval()
        with torch.no_grad():
            if not isinstance(X_batch, torch.Tensor):
                X_tensor = torch.tensor(X_batch, dtype=torch.float32)
            else:
                X_tensor = X_batch

            original_predictions = self.model(X_tensor).squeeze().cpu().numpy()
            shap_values = []
            X_numpy = X_tensor.cpu().numpy()

            for feature_index in range(X_numpy.shape[1]):
                X_masked = X_numpy.copy()
                X_masked[:, feature_index] = baseline[feature_index]

                X_masked_tensor = torch.tensor(X_masked, dtype=torch.float32)
                masked_predictions = self.model(X_masked_tensor).squeeze().cpu().numpy()

                if np.isscalar(original_predictions) and np.isscalar(masked_predictions):
                    feature_importance = abs(original_predictions - masked_predictions)
                else:
                    feature_importance = np.mean(np.abs(original_predictions - masked_predictions))

                shap_values.append(feature_importance)

        return np.array(shap_values)
