"""# Fifth dataset

## –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞

–ò—Å—Ç–æ—á–Ω–∏–∫ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ :
https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009
"""

# –ò–º–ø–æ—Ä—Ç—ã
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∫—Ä–∞—Å–∏–≤—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤
plt.style.use('seaborn-v0_8')
plt.rcParams['figure.figsize'] = (10, 6)

import torch
import time
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix
from torch.utils.data import DataLoader, TensorDataset

!pip install --no-deps xanfis mealpy permetrics

from xanfis import BioAnfisClassifier
from scipy.stats import pearsonr

# === –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ ===
data = pd.read_csv("./winequality-red.csv")

data.head()

# === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π ===
X = data.iloc[:, :-1].copy()
y = data['quality']
feature_names = X.columns.tolist()

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ train/test
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.25, random_state=42, stratify=y
)

print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
print(f"   Train: {X_train.shape[0]} –æ–±—ä–µ–∫—Ç–æ–≤")
print(f"   Test:  {X_test.shape[0]} –æ–±—ä–µ–∫—Ç–æ–≤")
print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[1]}")

"""## –í–∞–Ω–∏–ª—å–Ω–∞—è –ê–Ω—Ñ–∏—Å"""

# ===============================================================================
# –û–±—É—á–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ ANFIS —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# ===============================================================================
import seaborn as sns
import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (accuracy_score, roc_auc_score, confusion_matrix,
                           roc_curve, precision_recall_curve, average_precision_score,
                           precision_score, recall_score, f1_score)
import pandas as pd
from xanfis import BioAnfisRegressor

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
plt.style.use('default')
sns.set_palette("husl")

print("üîµ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ ANFIS...")
start_time = time.time()

# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ ANFIS
model_vanilla = BioAnfisRegressor(
    num_rules=33,                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ—á—ë—Ç–∫–∏—Ö –ø—Ä–∞–≤–∏–ª
    mf_class='Sigmoid',              # –¢–∏–ø —Ñ—É–Ω–∫—Ü–∏–π –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏
    optim='BaseGA',           # –ê–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (—Ä–æ–µ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
    optim_params={
        'epoch': 100,               # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
        'pop_size': 30,            # –†–∞–∑–º–µ—Ä –ø–æ–ø—É–ª—è—Ü–∏–∏ –¥–ª—è PSO
        'verbose': True           # –û—Ç–∫–ª—é—á–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ PSO
    },
    reg_lambda=0.1,                # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    seed=42,                       # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–µ—Ä–Ω–æ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    verbose=True
)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
print("üìö –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è...")
model_vanilla.fit(X_train, y_train)
vanilla_time = time.time() - start_time

# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
from sklearn.metrics import (
    confusion_matrix,
    mean_squared_error, mean_absolute_error,r2_score)
print("üîç –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
y_pred_vanilla = model_vanilla.predict(X_test)
model_vanilla.network.eval()
#y_prob_vanilla = model_vanilla.predict_proba(X_test)

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞


from sklearn.metrics import precision_score, recall_score, f1_score
import numpy as np

num_classes = len(np.unique(y_test))

rmse = np.sqrt(mean_squared_error(y_test, y_pred_vanilla))
mae  = mean_absolute_error(y_test, y_pred_vanilla)
r2   = r2_score(y_test, y_pred_vanilla)

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏
model_coefficients = model_vanilla.network.state_dict()['coeffs'].detach().cpu().numpy()
feature_importance_vanilla = np.sum(np.abs(model_coefficients[:, :-1, 0]), axis=0)

# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ ANFIS –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
print(f"   üìä RMSE: {rmse:.4f}")
print(f"   üéØ MAE: {mae:.4f}")
print(f"   üìà R2: {r2:.4f}")

print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {vanilla_time:.2f} —Å–µ–∫—É–Ω–¥")

plt.scatter(y_test, y_pred_vanilla, s=40, alpha=0.8, color='deepskyblue', label="–ü–∞—Ä—ã", zorder=1)
plt.scatter(y_test, y_test, s=40, alpha=0.3, color='limegreen', label="y = x", zorder=2)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=1.5)
plt.xlabel('–†–µ–∞–ª—å–Ω—ã–µ')
plt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ')
plt.title('Scatter')
plt.legend()

# ===============================================================================
# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# ===============================================================================
alpha=0.8
plt.scatter(range(len(y_test)), y_test, color='green',alpha=alpha, label="–†–µ–∞–ª—å–Ω—ã–µ", s=40)
plt.scatter(range(len(y_pred_vanilla)), y_pred_vanilla, color='red',alpha=alpha ,label="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ", marker='x', s=40)
plt.xlabel('–ò–Ω–¥–µ–∫—Å')
plt.ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
plt.title('Scatter: –†–µ–∞–ª—å–Ω—ã–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –ø–æ –∏–Ω–¥–µ–∫—Å—É')
plt.legend()

"""## SHAP Post-hoc"""

!pip install shap

# ===============================================================================
# –Ø–ß–ï–ô–ö–ê 5: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ SHAP Post-hoc –¥–ª—è vanilla –º–æ–¥–µ–ª–∏
# ===============================================================================
import shap
import time

def predict_fn(data):
  model_vanilla.network.eval()
  return np.asarray(model_vanilla.predict(data)).ravel()
print("üü¢ –í—ã—á–∏—Å–ª—è—é SHAP —á–µ—Ä–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫—É shap...")
start_time = time.time()

explainer = shap.KernelExplainer(predict_fn, X_test[:100])
shap_values = explainer.shap_values(X_test[:100])
posthoc_time = vanilla_time + time.time() - start_time  # –í—Ä–µ–º—è —Ä–∞—Å—á—ë—Ç–∞
shap_posthoc = np.abs(shap_values).mean(axis=0)  # –ì–ª–æ–±–∞–ª—å–Ω–∞—è (—Å—Ä–µ–¥–Ω—è—è) –≤–∞–∂–Ω–æ—Å—Ç—å
print(f"‚úÖ SHAP Post-hoc –≤—ã—á–∏—Å–ª–µ–Ω: —Ä–∞–∑–º–µ—Ä={shap_values.shape}, –≤—Ä–µ–º—è={posthoc_time:.2f} c.")

shap.summary_plot(shap_values, X_test[:100], feature_names=feature_names)

shap.summary_plot(shap_values, X_test[:100],
                  feature_names=feature_names, plot_type="bar")

shap.decision_plot(explainer.expected_value, shap_values,
                   X_test[:100], feature_names=feature_names,
                   feature_display_range=slice(None))

#local
X_sample_rounded = np.round(X_test[0], 2)
shap.force_plot(explainer.expected_value,
                shap_values[0],
                X_sample_rounded,
                feature_names=feature_names,
                matplotlib=True)

shap.plots.waterfall(shap.Explanation(values=shap_values[0],
                                      base_values=explainer.expected_value,
                                      data=X_test[0],
                                      feature_names=feature_names))

"""## –û–±—É—á–µ–Ω–∏–µ —Å Shap –ø–æ–¥–∫—Ä–µ–ª–ø–µ–Ω–∏–µ–º"""

import time
import torch
import numpy as np
from torch.utils.data import DataLoader, TensorDataset

class ShapAwareANFISTrainer:
    """
    –û–±—É—á–µ–Ω–∏–µ ANFIS —Å –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–π SHAP-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π.
    """

    def __init__(self, model, gamma=0.15, verbose=True):
        self.model = model.network
        self.gamma = gamma
        self.verbose = verbose
        self.training_time = 0

    def _calculate_shap_approximation(self, X_batch, baseline):
        self.model.eval()
        X_tensor = X_batch if isinstance(X_batch, torch.Tensor) else torch.tensor(X_batch, dtype=torch.float32)

        original_predictions = self.model(X_tensor).squeeze()

        shap_values = []
        for feature_index in range(X_tensor.shape[1]):
            X_masked = X_tensor.clone()
            X_masked[:, feature_index] = baseline[feature_index]

            masked_predictions = self.model(X_masked).squeeze()

            feature_importance = torch.mean(torch.abs(original_predictions - masked_predictions))
            shap_values.append(feature_importance)

        return torch.stack(shap_values)

    def fit(self, X_train, y_train, epochs=25, batch_size=32, lr=0.005):
        start_time = time.time()

        X_tensor = torch.tensor(X_train, dtype=torch.float32)
        y_tensor = torch.tensor(y_train, dtype=torch.float32)
        dataset = TensorDataset(X_tensor, y_tensor)
        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        baseline = torch.mean(X_tensor, dim=0)

        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)
        loss_fn = torch.nn.MSELoss()

        training_history = {'total_loss': [], 'main_loss': [], 'shap_loss': []}

        if self.verbose:
            print("–ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ ANFIS —Å SHAP-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π...")

        for epoch in range(epochs):
            epoch_total_loss = []
            epoch_main_loss = []
            epoch_shap_loss = []

            for batch_X, batch_y in data_loader:
                optimizer.zero_grad()
                self.model.train()

                predictions = self.model(batch_X).squeeze()
                main_loss = loss_fn(predictions, batch_y)

                shap_importance = self._calculate_shap_approximation(batch_X, baseline)
                shap_normalized = shap_importance / (torch.sum(shap_importance) + 1e-8)
                target_uniform = torch.ones_like(shap_normalized) / shap_normalized.numel()

                shap_loss = torch.mean((shap_normalized - target_uniform) ** 2)

                total_loss = main_loss + self.gamma * shap_loss
                total_loss.backward()
                optimizer.step()

                epoch_total_loss.append(total_loss.item())
                epoch_main_loss.append(main_loss.item())
                epoch_shap_loss.append(shap_loss.item())

            training_history['total_loss'].append(np.mean(epoch_total_loss))
            training_history['main_loss'].append(np.mean(epoch_main_loss))
            training_history['shap_loss'].append(np.mean(epoch_shap_loss))

            if self.verbose and (epoch + 1) % 5 == 0:
                print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}: "
                      f"–û–±—â–∞—è –ø–æ—Ç–µ—Ä—è: {training_history['total_loss'][-1]:.6f}, "
                      f"–û—Å–Ω–æ–≤–Ω–∞—è –ø–æ—Ç–µ—Ä—è: {training_history['main_loss'][-1]:.6f}, "
                      f"SHAP —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è: {training_history['shap_loss'][-1]:.6f}")

        self.training_time = time.time() - start_time
        if self.verbose:
            print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {self.training_time:.2f} —Å–µ–∫—É–Ω–¥")

        return training_history

    def predict(self, X_test):
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.tensor(X_test, dtype=torch.float32)
            predictions = self.model(X_tensor).squeeze().cpu().numpy()
        return predictions

    def get_global_shap_importance(self, X_sample):
        baseline = torch.mean(torch.tensor(X_sample, dtype=torch.float32), dim=0)
        return self._calculate_shap_approximation(torch.tensor(X_sample, dtype=torch.float32), baseline).detach().cpu().numpy()

print("üü† –û–±—É—á–µ–Ω–∏–µ ANFIS —Å SHAP-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π...")
import time
# –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
model_shapreg = BioAnfisRegressor(
    num_rules=7,                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ—á—ë—Ç–∫–∏—Ö –ø—Ä–∞–≤–∏–ª
    mf_class='Sigmoid',              # –¢–∏–ø —Ñ—É–Ω–∫—Ü–∏–π –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏
    optim='BaseGA',           # –ê–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (—Ä–æ–µ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
    optim_params={
        'epoch': 1,               # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
        'pop_size': 30,            # –†–∞–∑–º–µ—Ä –ø–æ–ø—É–ª—è—Ü–∏–∏ –¥–ª—è PSO
        'verbose': False          # –û—Ç–∫–ª—é—á–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ PSO
    },
    reg_lambda=0.1,                # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    seed=42,                       # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–µ—Ä–Ω–æ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    verbose=True
)
model_shapreg.fit(X_train[:50], y_train.values[:50])  # –º–∏–Ω–∏-–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
trainer = ShapAwareANFISTrainer(model_shapreg, gamma=0.5, verbose=True)

start_time = time.time()
trainer.fit(X_train, y_train.values, epochs=25)
shapreg_time = time.time() - start_time

# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
from sklearn.metrics import (
    confusion_matrix,
    mean_squared_error, mean_absolute_error,r2_score)
print("üîç –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
y_pred_shapreg = model_shapreg.predict(X_test)
model_shapreg.network.eval()

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞


from sklearn.metrics import precision_score, recall_score, f1_score
import numpy as np

num_classes = len(np.unique(y_test))

rmse = np.sqrt(mean_squared_error(y_test, y_pred_shapreg))
mae  = mean_absolute_error(y_test, y_pred_shapreg)
r2   = r2_score(y_test, y_pred_shapreg)

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏
model_coefficients = model_shapreg.network.state_dict()['coeffs'].detach().cpu().numpy()
feature_importance_shapreg = np.sum(np.abs(model_coefficients[:, :-1, 0]), axis=0)

# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
print(f"‚úÖ SHAP-—Ä–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–µ ANFIS –æ–±—É—á–µ–Ω–∏–µ:")
print(f"   üìä RMSE: {rmse:.4f}")
print(f"   üéØ MAE: {mae:.4f}")
print(f"   üìà R2: {r2:.4f}")

print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {shapreg_time:.2f} —Å–µ–∫—É–Ω–¥")